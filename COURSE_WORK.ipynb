{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7464681,"sourceType":"datasetVersion","datasetId":4345009}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Автогенерация текстовых описаний к видео (кейс Rutube)\n\nВ данном кейсе вам предлагается решить задачу автогенерации краткого тектового описания к видео, на основе видеофайла и автоматической транскрибации. \n\nСтруктура датасета следующая:\n\ntrain.csv\n- **video_name** - название видео (в директории **train_video**)\n- **stt_name** - название файла с транскрибацией (в директории **train_stt**)\n- **category_name** - категория видео\n- **title** - название видео\n- **description** - описание видео\n\nВ ноутбуке вы можете пронаблюдать baseline модель, без обучения (unsupervised) в качестве простого примера, основанную только на файле транскрибации. Также в конце считается метрика meteor по baseline модели и модели, которая из транскрипта речи (STT) выдает первые несколько предложений для сравнения. \n\nТестовый датасет будет прислан вам позднее, поэтому здесь он фигурировать не будет.","metadata":{}},{"cell_type":"markdown","source":"Немного про модель: LexRankSummarizer, не вдаваясь в детали, можно сказать, что модель основана на статистиках, ее цель - найти самые \"важные\" предложения из полного текста (STT). \n\nПредложения представляются в виде мешка слов и получают эмбеддинги c tfidf, далее считаются косинусные близости предложений друг с другом. Следующая часть модели взята из немалоизвестной PageRank - строится граф, где на рёбрах стоит косинусная близость. Финальная часть  - по графу строится матрица, в ней находится максимальное сингулярное значение и таким образом находятся самые \"значимые\" предложения из большого текста.\n\nПодробнее можно почитать например тут https://www.codingninjas.com/studio/library/lexrank\n\nНа метрики и сравнение моделей на других бенчмарках тут https://www.dialog-21.ru/media/5764/golovizninavspluskotelnikovev038.pdf\n\nДля предобработки данных мы только удаляем стоп-слова (слишком часто встречаемые, например предлоги, союзы и тп), которые могут портить модель.","metadata":{}},{"cell_type":"markdown","source":"## Предобученная T5","metadata":{}},{"cell_type":"code","source":"working_dir = '/kaggle/input/rut-data/rutube_hackathon_summary_generation_novosibirsk/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n\ndataset_train = pd.read_csv(os.path.join(working_dir, \"train\", \"train.csv\"))\ndataset_test = pd.read_csv(os.path.join(working_dir, \"test\", \"test.csv\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\n\ndef del_timestamps(text):\n    text = text.split(\"]  \")[1:]\n    return \" \".join(text)\n\ndef cut(lines: str, max_words=512):\n    if len(lines.split()) > max_words:\n        lines = \" \".join(lines.split()[:max_words])\n    return lines\n    \n\n\ndef gen_description(stt_name, n_sent, category_name, mode='train'):\n\n    with open(os.path.join(working_dir, mode, f'{mode}_stt', stt_name), 'r') as f:\n        lines = f.readlines()\n        lines = [del_timestamps(line.strip()) for line in lines]\n        lines = \" \".join(lines)\n        res = cut(lines)\n        if len(res)>0:\n            return res\n        else:\n            return category_name\n        \n        \ndataset_train['stt_sum'] = np.nan\ndataset_train['stt_sum'] = dataset_train.progress_apply(lambda l: gen_description(l.stt_name, 4, l.category_name, 'train'), axis=1)\n\ndataset_test['stt_sum'] = np.nan\ndataset_test['stt_sum'] = dataset_test.progress_apply(lambda l: gen_description(l.stt_name, 4, l.category_name, 'test'), axis=1)\n\ndataset_train, dataset_valid = train_test_split(dataset_train, test_size=0.1, shuffle=True)\nprint(f'Train: {len(dataset_train)}\\nValid: {len(dataset_valid)}\\nTest: {len(dataset_test)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U transformers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pprint\nimport numpy as np\n \nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL = 'cointegrated/rut5-base-multitask'\nBATCH_SIZE = 64\nEPOCHS = 2\nMAX_LENGTH = 512","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(MODEL)\n \n# Function to convert text data into model inputs and targets\ndef preprocess_function(examples):\n    inputs = [f\"summarize: {article}\" for article in examples['stt_sum']]\n    tokenized_inputs = tokenizer(\n        inputs,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding='max_length',  \n        return_tensors=\"pt\"\n    )\n \n    # Set up the tokenizer for targets\n    targets = [summary for summary in examples['description']]\n    with tokenizer.as_target_tokenizer():\n        tokenized_targets = tokenizer(\n            targets,\n            max_length=MAX_LENGTH,\n            truncation=True,\n            padding='max_length',\n            return_tensors=\"pt\"\n        ).input_ids\n \n    return tokenized_inputs.input_ids, tokenized_targets, tokenized_inputs.attention_mask\n \ntokenized_train_inputs, tokenized_train_targets, attention_mask_train = preprocess_function(dataset_train)\ntokenized_val_inputs, tokenized_val_targets, attention_mask_val = preprocess_function(dataset_valid)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, inputs, targets, attn):\n        self.inputs, self.targets, self.attn = inputs, targets, attn\n\n\n    def __len__(self):\n        return len(self.inputs)\n\n    \n    def __getitem__(self, idx):\n        return (self.inputs[idx], self.targets[idx], self.attn[idx])\n    \ntrain_custom_dataset = CustomDataset(tokenized_train_inputs, tokenized_train_targets, attention_mask_train)\nval_custom_dataset = CustomDataset(tokenized_val_inputs, tokenized_val_targets, attention_mask_val)\n\n\ntrain_dataloader = DataLoader(train_custom_dataset, batch_size=8)\nval_dataloader = DataLoader(val_custom_dataset, batch_size=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n@torch.no_grad()\ndef evaluate(model, criterion, val_loader):\n    model.eval()\n    losses = 0\n    for src, tgt, attn in tqdm(val_loader, total=len(list(val_dataloader))):\n        src = src.to(device)\n        tgt = tgt.to(device)\n        attn = attn.to(device)\n        \n        tgt[tgt == tokenizer.pad_token_id] = -100\n        loss = model(input_ids=src, labels=tgt, attention_mask=attn).loss        \n        \n        losses += loss.item()\n        torch.cuda.empty_cache()\n    return losses / len(list(val_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndef train_epoch(model, optimizer, train_loader):\n    model.train()\n    losses = 0\n    for src, tgt, attn in tqdm(train_dataloader, total=len(list(train_loader))):            \n    \n        tgt[tgt == tokenizer.pad_token_id] = -100\n        \n        src = src.to(device)\n        tgt = tgt.to(device)\n        attn = attn.to(device)\n        loss = model(input_ids=src, labels=tgt, attention_mask=attn).loss\n        print(loss)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        \n        losses += loss.item()\n        torch.cuda.empty_cache()\n        del src\n        del tgt\n        del attn\n        gc.collect()\n    return losses / len(list(train_loader))\n\n\ndef train(model, optimizer, scheduler, train_loader, val_loader, n_epochs):\n    train_loss = 0.0\n    val_loss = 0.0\n    for epoch in range(n_epochs):\n        train_loss = train_epoch(model, optimizer, train_loader)\n        val_loss = evaluate(model, val_loader)\n        print(f\"Epoch {epoch}\\ntrain loss: {train_loss}\\nval loss: {val_loss}\\n\")\n\n        if scheduler is not None:\n            scheduler.step(val_loss)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(MODEL)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ntrain(model, optimizer, None, train_dataloader, val_dataloader, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained(MODEL)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL)\n\ninput_ids = tokenizer(\"Привет как дела\", return_tensors=\"pt\").input_ids\nlabels = tokenizer(\"Все хорошо\", return_tensors=\"pt\").input_ids\n\n# the forward function automatically creates the correct decoder_input_ids\nloss = model(input_ids=tokenized_train_inputs[:64, :], labels=tokenized_train_targets[:64, :]).loss\nloss.item()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Бейзлайны","metadata":{}},{"cell_type":"code","source":"working_dir = '/kaggle/input/rut-data/rutube_hackathon_summary_generation_novosibirsk/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lex rank - unsupervised upproach\nfrom sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.lex_rank import LexRankSummarizer\nfrom sumy.nlp.stemmers import Stemmer\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np\n\n\n# nltk.download('stopwords')\n\n# Допольнительные стоп-слова можно скачать здесь\n# https://github.com/stopwords-iso/stopwords-ru/blob/master/raw/stop-words-russian.txt\n# но в этот список мы также добавили пару примеров вручную, поэтому прикладываем готовый файл. \n# Вы также можете модифицировать на свое усмотрение, или вовсе от него отказаться\n\nwith open(working_dir + \"stop-words-russian.txt\", 'r') as f:\n    extra_stop_words = f.readlines()\n    extra_stop_words = [line.strip() for line in extra_stop_words]\n\n\ndef sumy_method(text, n_sent: int = 4):\n    \n    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n    \n    stemmer = Stemmer(\"russian\")\n    summarizer = LexRankSummarizer(stemmer)\n    stopwords_ru = stopwords.words('russian')\n    stopwords_ru.extend(extra_stop_words)\n    summarizer.stop_words = stopwords_ru\n    \n    #Summarize the document with n_sent sentences\n    summary = summarizer(parser.document, n_sent)\n    dp = []\n    if len(summary)> 0:\n        for i in summary:\n            lp = str(i)\n            dp.append(lp)\n    \n        final_sentence = ' '.join(dp)\n    else:\n        final_sentence = ''\n    if len(final_sentence.split(\" \"))>512:\n        final_sentence = \" \".join(final_sentence.split(\" \")[:512])\n    return final_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nPATH_TO_DATA = working_dir + 'train/'\ndataset = pd.read_csv(os.path.join(PATH_TO_DATA, \"train.csv\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(os.path.join(PATH_TO_DATA, 'train_stt', '478.txt'), 'r') as f:\n        lines = f.readlines()\n        lines = [line.strip() for line in lines]\nlines","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Чтобы понять сколько предложений нам нужно выдавать в качестве описания, посчитаем статистики","metadata":{}},{"cell_type":"code","source":"import nltk\n# from nltk.translate import meteor\nfrom nltk.translate import meteor_score\nfrom nltk import word_tokenize, sent_tokenize\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['len'] = dataset.description.apply(lambda l : len(sent_tokenize(l)))\nprint(\"Среднее число предложений в трейн датасете\", np.mean(dataset['len'].to_list()))\nprint(\"Медиана\", np.median(dataset['len'].to_list()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Теперь поймём примерный размер в токенах","metadata":{}},{"cell_type":"code","source":"dataset['len_tokens'] = dataset.description.apply(lambda l : len(l.split(\" \")))\n\nprint(\"Среднее число слов в трейн датасете\", np.mean(dataset['len_tokens'].to_list()))\nprint(\"Медиана\", np.median(dataset['len_tokens'].to_list()))\nprint(\"Максимум\", np.max(dataset['len_tokens'].to_list()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# поэтому в sumy_method мы добавили ограничение на число слов в сгенерированном тексте \n# (512 слов в нашем случае, решили так ограничить макс 348 слов из трейна)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Генерируем текстовые описания для всех видео из трейна по текстовому описанию (из Speech To Text)\nЕсли в видео не было речи, то в качестве описания ставим категорию видео","metadata":{}},{"cell_type":"code","source":"# Очистим STT от временных кодов\nfrom tqdm import tqdm\ntqdm.pandas()\ndef del_timestamps(text):\n    text = text.split(\"]  \")[1:]\n    return \" \".join(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_description(stt_name, n_sent, category_name):\n\n    with open(os.path.join(PATH_TO_DATA, 'train_stt', stt_name), 'r') as f:\n        lines = f.readlines()\n        lines = [del_timestamps(line.strip()) for line in lines]\n        lines = \" \".join(lines)\n        res = sumy_method(lines, n_sent)\n        if len(res)>0:\n            return res\n        else:\n            return category_name\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndataset['stt_sum'] = np.nan\ndataset['stt_sum'] = dataset.progress_apply(lambda l: gen_description(l.stt_name, 4, l.category_name), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# видео, по которым нет речи и соответсвенно модель не смогла ничего выдать\ndataset[dataset.stt_sum.isin(dataset.category_name.unique())]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Посчитаем метрику meteor","metadata":{}},{"cell_type":"code","source":"def func(stt_name, text, text_sum):\n    if isinstance(text_sum, str):\n        return round(meteor_score.meteor_score([word_tokenize(text)],word_tokenize(text_sum)), 4)\n    else:\n        return 0\ndataset['meteor'] = dataset.apply(lambda l: func(l['stt_name'], l.description, l.stt_sum), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Значение метрики meteor для unsupervised модели\", dataset.meteor.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Сравним с моделью, которая выдает первые 4 предложения из STT","metadata":{}},{"cell_type":"code","source":"%%time\ndef func(stt_name, text, category_name):\n    with open(os.path.join(PATH_TO_DATA, 'train_stt', stt_name), 'r') as f:\n        lines = f.readlines()\n        lines = [del_timestamps(line.strip()) for line in lines]\n        res = lines[:4]\n    res = \" \".join(lines)\n    if isinstance(res, str):\n        return round(meteor_score.meteor_score([word_tokenize(text)],word_tokenize(res)), 4)\n    else:\n        return round(meteor_score.meteor_score([word_tokenize(text)],word_tokenize(category_name)), 4)\ndataset['meteor_first4'] = dataset.apply(lambda l: func(l['stt_name'], l.description, l.category_name), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Значение метрики meteor для модели, выдающей первые 4 предложения\", dataset.meteor_first4.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}